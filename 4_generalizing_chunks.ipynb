{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalizing Chunks\n",
    "#### John R. Starr; jrs294@pitt.edu\n",
    "Now that we have all of the data POS-tagged and chunked, it's time to generalize the chunks into categories: SOV, SVO, and an \"extraneous\" column EX (which will probably be filled by mis-parsed/mis-chunked data). At the most basic level, I intend on examining the number of noun phrases before the verb. If there are two, then we'll make it one. \n",
    "\n",
    "NOTE: I understand that the chunked data I have is by no means perfect; this is one of the limitations of my project.\n",
    "\n",
    "All right, let's load in the usual stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_pickle('tagged_chunked_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eng</th>\n",
       "      <th>Far</th>\n",
       "      <th>Eng_Tok</th>\n",
       "      <th>Far_Tok</th>\n",
       "      <th>Eng_Len</th>\n",
       "      <th>Far_Len</th>\n",
       "      <th>Eng_Types</th>\n",
       "      <th>Far_Types</th>\n",
       "      <th>Far_POS</th>\n",
       "      <th>Far_Chunks</th>\n",
       "      <th>Eng_POS</th>\n",
       "      <th>Eng_Chunks</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>raspy breathing</td>\n",
       "      <td>صداي خر خر</td>\n",
       "      <td>[raspy, breathing]</td>\n",
       "      <td>[صداي, خر, خر]</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>{breathing, raspy}</td>\n",
       "      <td>{خر, صداي}</td>\n",
       "      <td>[(صداي, NUM), (خر, Ne), (خر, N)]</td>\n",
       "      <td>[صداي خر خر NP]</td>\n",
       "      <td>[(raspy, NN), (breathing, NN)]</td>\n",
       "      <td>[[(raspy, NN), (breathing, NN)]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dad</td>\n",
       "      <td>پدر</td>\n",
       "      <td>[dad]</td>\n",
       "      <td>[پدر]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{dad}</td>\n",
       "      <td>{پدر}</td>\n",
       "      <td>[(پدر, N)]</td>\n",
       "      <td>[پدر NP]</td>\n",
       "      <td>[(dad, NN)]</td>\n",
       "      <td>[[(dad, NN)]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>maybe its the wind</td>\n",
       "      <td>شايد صداي باد باشه</td>\n",
       "      <td>[maybe, its, the, wind]</td>\n",
       "      <td>[شايد, صداي, باد, باشه]</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>{wind, its, maybe, the}</td>\n",
       "      <td>{باشه, باد, صداي, شايد}</td>\n",
       "      <td>[(شايد, Ne), (صداي, AJ), (باد, V), (باشه, V)]</td>\n",
       "      <td>[شايد صداي NP] [باد VP] [باشه VP]</td>\n",
       "      <td>[(maybe, RB), (its, PRP$), (the, DT), (wind, NN)]</td>\n",
       "      <td>[(maybe, RB), (its, PRP$), [(the, DT), (wind, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>no</td>\n",
       "      <td>نه</td>\n",
       "      <td>[no]</td>\n",
       "      <td>[نه]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>{no}</td>\n",
       "      <td>{نه}</td>\n",
       "      <td>[(نه, ADV)]</td>\n",
       "      <td>نه</td>\n",
       "      <td>[(no, DT)]</td>\n",
       "      <td>[[(no, DT)]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stop please stop</td>\n",
       "      <td>دست نگه داريد خواهش ميکنم دست نگه داريد</td>\n",
       "      <td>[stop, please, stop]</td>\n",
       "      <td>[دست, نگه, داريد, خواهش, ميکنم, دست, نگه, داريد]</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>{please, stop}</td>\n",
       "      <td>{ميکنم, نگه, خواهش, دست, داريد}</td>\n",
       "      <td>[(دست, N), (نگه, N), (داريد, V), (خواهش, Ne), ...</td>\n",
       "      <td>[دست NP] [نگه داريد VP] [خواهش ميکنم دست NP] [...</td>\n",
       "      <td>[(stop, JJ), (please, NN), (stop, VB)]</td>\n",
       "      <td>[[(stop, JJ), (please, NN)], [[('stop', 'VB')]]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Eng                                      Far  \\\n",
       "ID                                                                \n",
       "1      raspy breathing                               صداي خر خر   \n",
       "2                  dad                                      پدر   \n",
       "3   maybe its the wind                       شايد صداي باد باشه   \n",
       "4                   no                                       نه   \n",
       "5     stop please stop  دست نگه داريد خواهش ميکنم دست نگه داريد   \n",
       "\n",
       "                    Eng_Tok                                           Far_Tok  \\\n",
       "ID                                                                              \n",
       "1        [raspy, breathing]                                    [صداي, خر, خر]   \n",
       "2                     [dad]                                             [پدر]   \n",
       "3   [maybe, its, the, wind]                           [شايد, صداي, باد, باشه]   \n",
       "4                      [no]                                              [نه]   \n",
       "5      [stop, please, stop]  [دست, نگه, داريد, خواهش, ميکنم, دست, نگه, داريد]   \n",
       "\n",
       "    Eng_Len  Far_Len                Eng_Types  \\\n",
       "ID                                              \n",
       "1         2        3       {breathing, raspy}   \n",
       "2         1        1                    {dad}   \n",
       "3         4        4  {wind, its, maybe, the}   \n",
       "4         1        1                     {no}   \n",
       "5         3        8           {please, stop}   \n",
       "\n",
       "                          Far_Types  \\\n",
       "ID                                    \n",
       "1                        {خر, صداي}   \n",
       "2                             {پدر}   \n",
       "3           {باشه, باد, صداي, شايد}   \n",
       "4                              {نه}   \n",
       "5   {ميکنم, نگه, خواهش, دست, داريد}   \n",
       "\n",
       "                                              Far_POS  \\\n",
       "ID                                                      \n",
       "1                    [(صداي, NUM), (خر, Ne), (خر, N)]   \n",
       "2                                          [(پدر, N)]   \n",
       "3       [(شايد, Ne), (صداي, AJ), (باد, V), (باشه, V)]   \n",
       "4                                         [(نه, ADV)]   \n",
       "5   [(دست, N), (نگه, N), (داريد, V), (خواهش, Ne), ...   \n",
       "\n",
       "                                           Far_Chunks  \\\n",
       "ID                                                      \n",
       "1                                     [صداي خر خر NP]   \n",
       "2                                            [پدر NP]   \n",
       "3                   [شايد صداي NP] [باد VP] [باشه VP]   \n",
       "4                                                  نه   \n",
       "5   [دست NP] [نگه داريد VP] [خواهش ميکنم دست NP] [...   \n",
       "\n",
       "                                              Eng_POS  \\\n",
       "ID                                                      \n",
       "1                      [(raspy, NN), (breathing, NN)]   \n",
       "2                                         [(dad, NN)]   \n",
       "3   [(maybe, RB), (its, PRP$), (the, DT), (wind, NN)]   \n",
       "4                                          [(no, DT)]   \n",
       "5              [(stop, JJ), (please, NN), (stop, VB)]   \n",
       "\n",
       "                                           Eng_Chunks  \n",
       "ID                                                     \n",
       "1                    [[(raspy, NN), (breathing, NN)]]  \n",
       "2                                       [[(dad, NN)]]  \n",
       "3   [(maybe, RB), (its, PRP$), [(the, DT), (wind, ...  \n",
       "4                                        [[(no, DT)]]  \n",
       "5    [[(stop, JJ), (please, NN)], [[('stop', 'VB')]]]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than working on the full DF, let's create a smaller one that we can run functions on faster...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df = full_df.iloc[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what type of data we're working with. Because we're looking for the abnormalities in Persian word order, we only need to focus on that data. So, what's it all look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[صداي خر خر NP]\n",
      "[\n",
      "\n",
      "[پدر NP]\n",
      "[\n",
      "\n",
      "[شايد صداي NP] [باد VP] [باشه VP]\n",
      "[\n",
      "\n",
      "نه\n",
      "ن\n",
      "\n",
      "[دست NP] [نگه داريد VP] [خواهش ميکنم دست NP] [نگه داريد VP]\n",
      "[\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gathering some information on the Persian data:\n",
    "for item in small_df.Far_Chunks.iloc[:5]:\n",
    "    print(item)\n",
    "    print(item[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah.. it seems that our data is in string format...! This makes things a little more annoying to navigate. If only the data were in tuple form! Oh well...  Let's try using RegEx to search through this and get the word order that we want.\n",
    "\n",
    "NOTE: Persian reads from right-to-left; however, the chunker flips the sentence backward so that it can be read \"left-to-right\" and can be more easily compared with languages like English. If this is confusing now, I'll explain more in my presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Persian characters, getting stats on word orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1                      [ NP]]\n",
       "2                      [ NP]]\n",
       "3          [ NP],  VP],  VP]]\n",
       "4                          []\n",
       "5    [ NP],  VP],  NP],  VP]]\n",
       "Name: Far_Chunks, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test thing\n",
    "test_gen = small_df.Far_Chunks.apply(lambda x: re.findall(r' [A-Z]+P\\]', x))\n",
    "\n",
    "test_gen.head()\n",
    "len(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1                NP\n",
       "2                NP\n",
       "3        NP  VP  VP\n",
       "4                  \n",
       "5    NP  VP  NP  VP\n",
       "Name: Far_Chunks, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing extra stuff:\n",
    "test_gen = test_gen.apply(str).apply(lambda x: re.sub(r\"[,\\[\\]\\']\", '', x)).apply(lambda x: x.strip())\n",
    "test_gen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "1                      [ NP]]\n",
       "2                      [ NP]]\n",
       "3          [ NP],  VP],  VP]]\n",
       "4                          []\n",
       "5    [ NP],  VP],  NP],  VP]]\n",
       "Name: Far_Chunks, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ID\n",
       "612082                  [ NP]]\n",
       "612083                  [ NP]]\n",
       "612084    [ NP],  ADJP],  VP]]\n",
       "612085                  [ NP]]\n",
       "612086                  [ NP]]\n",
       "Name: Far_Chunks, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "612086"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Whole thing\n",
    "full_gen = full_df.Far_Chunks.apply(lambda x: re.findall(r' [A-Z]+P\\]', x))\n",
    "full_gen.head()\n",
    "full_gen.tail()\n",
    "len(full_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing brackets and lists\n",
    "full_gen = full_gen.apply(str).apply(lambda x: re.sub(r\"[,\\[\\]\\']\", '', x)).apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NP', 97417),\n",
       " ('NP  VP', 36201),\n",
       " ('NP  NP', 17290),\n",
       " ('VP', 14023),\n",
       " ('NP  NP  VP', 13694),\n",
       " ('NP  PP  NP  VP', 13342),\n",
       " ('NP  PP  NP', 11081),\n",
       " ('PP  NP', 8256),\n",
       " ('NP  VP  VP', 8138),\n",
       " ('PP  NP  VP', 7168),\n",
       " ('NP  VP  NP', 6901),\n",
       " ('NP  VP  NP  VP', 6462),\n",
       " ('NP  ADJP  VP', 6022),\n",
       " ('', 4384),\n",
       " ('ADVP  VP', 4176)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "chunk_tags_freq = Counter(full_gen)\n",
    "chunk_tags_freq.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VISUALIZATION?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, many of these files have complex structures that include multiple NPs and VPs. Unforuntately, the strings do not have punctuation, so I can't split the sentences up on that. Admittedly, this is kind of giving me a headache.\n",
    "\n",
    "But still, time to fix things!! Thanks to some help from Professor Han, I have tried something else that works better. Here are some sample strings to test on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'NP VP'\n",
    "s2 = 'NP VP NP'\n",
    "s3 = 'NP NP VP'\n",
    "s4 = 'NP NP VP NP'\n",
    "s5 = 'NP ADVP VP NP'\n",
    "s6 = 'NP ADVP NP VP'\n",
    "s7 = 'NP ADVP PP VP NP'\n",
    "s8 = 'NP ADVP PP NP VP'\n",
    "s9 = 'NP ADVP PP VP ADVP NP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll build structures that DO represent the necessary structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refine for pp!!\n",
    "np = r'NP'\n",
    "vp = r'VP'\n",
    "advp = r'ADVP'\n",
    "pp = r'PP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to build the SVO and SOV word order patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching for SVO\n",
    "svo_pat = re.compile(r\"\"+ np +\" ([\"+ advp + pp +\" ])*\" + vp + \" ([\"+ advp + pp +\" ])*\" + np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s2\n",
      "s5\n",
      "s6\n"
     ]
    }
   ],
   "source": [
    "# Testing match\n",
    "if svo_pat.match(s1):\n",
    "    print('s1')\n",
    "if svo_pat.match(s2):\n",
    "    print('s2')\n",
    "if svo_pat.match(s3):\n",
    "    print('s3')\n",
    "if svo_pat.match(s4):\n",
    "    print('s4')\n",
    "if svo_pat.match(s5):\n",
    "    print('s5')\n",
    "if svo_pat.match(s6):\n",
    "    print('s6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching for SOV\n",
    "sov_pat = re.compile( r\"\"+ np + ' ' + np + \" \" + vp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3\n",
      "s4\n"
     ]
    }
   ],
   "source": [
    "# Testing match\n",
    "if sov_pat.match(s1):\n",
    "    print('s1')\n",
    "if sov_pat.match(s2):\n",
    "    print('s2')\n",
    "if sov_pat.match(s3):\n",
    "    print('s3')\n",
    "if sov_pat.match(s4):\n",
    "    print('s4')\n",
    "if sov_pat.match(s5):\n",
    "    print('s5')\n",
    "if sov_pat.match(s6):\n",
    "    print('s6')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, let's see what changes when we use these two functions instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_word_order3(text):\n",
    "    if svo_pat.match(text):\n",
    "        return 'SVO'\n",
    "    elif sov_pat.match(text):\n",
    "        return 'SOV'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Other'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-274570019f7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdct\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'Other'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df['Word_Order3'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if I flip the function? Last time, this was a good way of checking whether or not the ordering of the if/elif/else statements was affecting the numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_word_order4(text):\n",
    "    if sov_pat.match(text):\n",
    "        return 'SOV'\n",
    "    elif svo_pat.match(text):\n",
    "        return 'SVO'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df['Word_Order4'] = small_df['Far_Chunks'].apply(gen_word_order4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_df['Word_Order4'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, at least this one doesn't have any cross-over! And, as expected, there are more SOV structures.\n",
    "\n",
    "Unfortunately, this function ignores 75% of the small_df. That's a _lot_ of data to be losing. We'll need to see how this scales up when applied to the full DF.\n",
    "\n",
    "A quick refresher on what our full DF looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And applying our word_order function to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_word_order_final(text):\n",
    "    if sov_pat.match(text):\n",
    "        return 'SOV'\n",
    "    elif svo_pat.match(text):\n",
    "        return 'SVO'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['Word_Order'] = full_df['Far_Chunks'].apply(gen_word_order_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_df['Word_Order'].value_counts())\n",
    "print()\n",
    "print(len(full_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_df = pd.DataFrame({'Word_Order': [535371, 52756 , 23959]},\n",
    "                        index=['Other', 'SOV', 'SVO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = count_df.plot.pie(y='Word_Order', figsize=(5, 5))\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this is _not_ ideal whatsoever. Over 87% of the data is labeled as \"other\". There are a few explanations for this:\n",
    "- Chunker mischunked the data, therefore giving incorrect structures\n",
    "- Regex are too restrictive in their selection (need editing)\n",
    "- Various factors do not take into account the pro-drop nature of Persian, so they don't recognize OV structures to be SOV (create a separate OV category?)\n",
    "\n",
    "Due to time constraints, I don't think I'll be able to fix this right now, but hopefully by the time I present on Tuesday I'll have a better grasp on my data...! For now, let's separate the structures that are properly labeled, pickle them out, then perform data analysis in another notebook.\n",
    "\n",
    "First, I'll create a list of Boolean values that will distinguish between the \"Other\" and ordered cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "booleans = []\n",
    "for order in full_df['Word_Order']:\n",
    "    if order == 'Other':\n",
    "        booleans.append(False)\n",
    "    else:\n",
    "        booleans.append(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the length\n",
    "len(booleans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll turn this list into a Series so that way we can use it to organize the DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ordered = pd.Series(booleans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait! The index for full_df starts at 1, not 0. Let's reset that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to create the \"ordered only\" DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_only_df = full_df[is_ordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_only_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_only_df.to_pickle('ordered_only_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_only_df = full_df[full_df['Word_Order'] == 'Other']\n",
    "len(other_only_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_only_df.to_pickle('other_only_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it for now...! Time for some data analysis!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
